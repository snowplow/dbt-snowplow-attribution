snowplow-attribution 0.5.0 (2025-04-16)
---------------------------------------
## Summary
This release enables passing multiple path transformations as dictionary values for transformations enabled in `dbt_project.yml` using the variable `snowplow__path_transforms`.

It is specifically related to transformations `remove_if_not_all` and `remove_if_last_and_not_all`.

### Usage example
```yml
snowplow__path_transforms: {'exposure_path': null, 'remove_if_not_all': ['placeholder_transformation_1', 'placeholder_transformation_2'], 'remove_if_last_and_not_all': ['placeholder_transformation_3', 'placeholder_transformation_4']}
```

## Features
- Allow multiple path transformation parameters

## Fixes
- Fix warning message in source checks macro
- Fix trim_long_path transformation

## ðŸš¨ Breaking Changes ðŸš¨
- From now on the `snowplow__path_transforms` variable parameters only accept non-empty arrays for `remove_if_last_and_not_all` and `remove_if_not_all` variables instead of strings, please your variable overwrites in your dbt_project.yml accordingly. Previously you could only remove one specific channel or campaign, now you can do multiple, if needed.

```yml title="dbt_project.yml"
vars:
  snowplow_attribution:
    snowplow__path_transforms: {'exposure_path': null, 'remove_if_last_and_not_all': ['channel_to_remove_1', 'campaign_to_remove_1', 'campaign_to_remove_2']}
  ```

## Upgrading
Update the snowplow-attribution version in your `packages.yml` file. For existing users, please follow the migration guide [here](https://docs.snowplow.io/docs/modeling-your-data/modeling-your-data-with-dbt/migration-guides/attribution/#upgrading-to-050).


snowplow-attribution 0.4.0 (2024-10-15)
---------------------------------------
## Summary
This release extends support to Apache Spark with the Iceberg file format and updates integration tests accordingly.

## Features
- Add support for Apache Spark with Iceberg file format

## Under the hood
- Fix return_limit_from_model macro 

## Upgrading
Update the snowplow-attribution version in your `packages.yml` file.


snowplow-attribution 0.3.0 (2024-07-26)
---------------------------------------
## Summary
This release introduces a new user stitching logic when the `snowplow__conversion_stitching` variable is not enabled to avoid the need of having to enable both view and conversion stitching in the unified package to get the most accurate user journeys and conversions. The package will rely on one more source from the Unified Digital dbt package, the `snowplow_unified_user_mapping` table to accomplish this.

## Features
Change stitching logic

## Upgrading
Bump the snowplow-attribution version in your `packages.yml` file.

## ðŸš¨ Breaking Changes ðŸš¨
Due to the new user mapping table join in the paths_to_conversions() macro it may be that your `snowplow__conversion_clause` variable would need to be changed by adding the `ev` table alias to the fields it references (for user_identifier or user_id).

The package by default will now rely on the `snowplow_unified_user_mapping` table. Although most users would use the Unified package as a source already (therefore this should not be a breaking change), for those users where it is not available, the `paths_to_conversion()` macro will have to be overwritten in the dbt project where the package is referenced. Similarly, the optional `paths_to_non_conversion` model is also changed, it would need to be disabled and overwritten in that case, too.

snowplow-attribution 0.2.2 (2024-06-19)
---------------------------------------
## Summary
This release fixes a bug in the source_checks() macro that could fail if the `snowplow__conversions_source` or `snowplow__conversion_path_source` variables are overwritten. This fix should allow users to provide a `schema.table` or `warehouse.schema.table` style string reference for these sources without having to specify a dbt source in their project (similar to the default), if necessary.

## Fixes
- Fix source checks macro

## Upgrading
Bump the snowplow-attribution version in your `packages.yml` file.

snowplow-attribution 0.2.1 (2024-06-11)
---------------------------------------
## Summary
This release removes null user_identifier data when the conversions_source and path_source are joined. Previously catching nulls on this field was reliant on tests only in the Unified package which was removed as it is easier to enforce this directly in the model itself.

## Features
- Exclude null user identifiers from sources

## Under the hood
- Add Redshift to tests

## Upgrading
Bump the snowplow-attribution version in your `packages.yml` file.

snowplow-attribution 0.2.0 (2024-03-26)
---------------------------------------
## Summary
This release adds new `cv_id` and `cv_type` columns to align with the latest snowplow_unified package's conversion table structure which this package may take as a base (recommended). It also makes a few internal tweaks, including adding support for [schema grants](https://docs.snowplow.io/docs/modeling-your-data/modeling-your-data-with-dbt/package-features/table-grants/#granting-usage-on-schemas).

## ðŸš¨ Breaking Changes ðŸš¨
- The surrogate_key as well as the update column has changed, it is best to do a [full-refresh](https://docs.snowplow.io/docs/modeling-your-data/modeling-your-data-with-dbt/dbt-operation/full-or-partial-refreshes/) as part of the upgrade.

## Features
- Add support for schema grants
- Add cv_id and cv_type

## Under the hood
- Enforce full refresh flag for refreshing manifest tables
- Safeguard spend source from duplication
- Disable reporting models

## Upgrading
To upgrade bump the snowplow-attribution version in your `packages.yml` file and run a [full refresh](https://docs.snowplow.io/docs/modeling-your-data/modeling-your-data-with-dbt/dbt-operation/full-or-partial-refreshes/)

Although there are no direct dependencies between the packages, the [snowplow-unified](https://docs.snowplow.io/docs/modeling-your-data/modeling-your-data-with-dbt/dbt-models/dbt-unified-data-model/) package needs to be upgraded as well to [v.0.4.0](https://github.com/snowplow/dbt-snowplow-unified/releases/tag/0.4.0) for the new package structure to work.

snowplow-attribution 0.1.0 (2024-01-31)
---------------------------------------

## Summary
This is the first release of the Snowplow Attribution package, which contains incremental tables to prepare data for marketing attribution analysis as well as report tables which help you understand and visualize which channels or campaigns attributed the most to your revenue. It supports various attibution models out of the box, all in SQL.

## Features
- incremental dbt Package that produces tables for marketing attribution analysis
- Support for Snowflake / BigQuery / Databricks / Redshift

## Installation
To install the package, add the following to the `packages.yml` in your project:

### Github
```yml
packages:
  - git: "https://github.com/snowplow/dbt-snowplow-attribution.git"
    revision: 0.1.0
```
### dbt hub
Please note that it may be a few days before the package is available on dbt hub after the initial release.
```yml
packages:
  - package: snowplow/snowplow_attribution
    version: [">=0.1.0", "<0.2.0"]
